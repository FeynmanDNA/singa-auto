
# Inference
RUNNING_INFERENCE_WORKERS = 'running__inference_workers'
REQUEST_QUEUE = 'request_queue'
QFE_SLEEP = 0.25
INFERENCE_WORKER_SLEEP = 0.25
BATCH_SIZE = 32
