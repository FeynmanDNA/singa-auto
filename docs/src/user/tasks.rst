 .. _`tasks`:

Supported Tasks
====================================================================

Each task has an associated a *Dataset Format*, a *Query Format* and a *Prediction Format*.

A task's *Dataset Format* specifies the format of the dataset files.
Datasets are prepared by *Application Developers* when they create *Train Jobs*
and received by *Model Developers* when they define :meth:`singa_auto.model.BaseModel.train` and :meth:`singa_auto.model.BaseModel.evaluate`.

A task's *Query Format* specifies the format of queries when they are passed to models. 
Queries are generated by *Application Users* when they send queries to *Inference Jobs* 
and received by *Model Developers* when they define :meth:`singa_auto.model.BaseModel.predict`.

A task's *Prediction Format* specifies the format of predictions made by models. 
Predictions are generated by *Model Developers* when they define :meth:`singa_auto.model.BaseModel.predict`
and received by *Application Users* as predictions to their queries sent to *Inference Jobs*.


QUESTION_ANSWERING
--------------------------------------------------------------------


Dataset Format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:QUESTION_ANSWERING`


Dataset can be used to finetune the SQuAD pre-trained Bert model. 

- The dataset zips folders containing JSON files. JSON files under different level folders will be automaticly read all together. 

Dataset structure example #1:

.. code-block:: text

    /DATASET_NAME.zip
    │
    ├──FOLDER_NAME_1                                              # first level folder
    │  └──FOLDER_NAME_2                                           # second level folder, not necessarily to be included
    │      └──FOLDER_NAME_3                                       # third level folder, not necessarily to be included
    │           ├── 003d2e515e1aaf06f0052769953e8.json            # JSON file name is a random combination of either alphabets/numbers or both
    │           ├── 00a407540a8bdd.json
    │           ...
    │
    ├──FOLDER_NAME_4                                              # first level folder
    │  ├── 0015023cc06b5362d332b3.json
    │  ├── 001b4a31684c8fc6e2cfbb70304354978317c429.json
    │  ...
    ...
    │
    └──metadata.csv                                               # if additional information is provided for above JSON files, user can add a metadata.csv

- JSON file includes ``body_text``, providing list of paragraphs in full body which can be used for question answering. ``body_text`` can contain different entries, only the "text" field of each entry will be read. 

1. For JSON files extracted from papers, it comes that one JSON file for one paper. And if additional information is given in metadata.csv for papers, each JSON file and each metadata.csv entries are linked via ``sha`` values of both.

2. For dataset having their additional information paragraph, the ``body_text``> ``text`` entry is in ``<question> + <\n> + <information paragraph>`` string format. In this circumstance, there is no ``sha`` value nor metadata.csv file needed.

Sample of JSON file:

.. code-block:: text

    # JSON file 1                           # for example, a JSON file extracted from one paper
    {
        "sha": <str>,                       # 40-character sha1 of the PDF, this field is only required for JSON extracted from papers
        
        "body_text": [                      # list of paragraphs in full body, this is must-have
            {                               
                "text": <str>,              # text body for first entry, which is for one paragraph. this is must-have 
            },
            ...                             # other entries, paragraph dicts look the same as above
        ],
    }
    
    # ---------------------------------------------------------------------------------------------------------------------- #
    
    # JSON file 2                           # for example, a JSON file extraced from SQuAD2.0
    {        
        "body_text": [                      # list of paragraphs in full body, this is must-have
            {                               
                "text": 'What are the treatments for Age-related Macular Degeneration ?\n If You Have Advanced AMD Once dry AMD reaches the advanced stage, no form of treatment can prevent vision loss...',              
                                            # text body for first entry, this is must-have 
                                            
            },
            ...                             # other entries, paragraph dicts look the same as above
        ],
    }
    

- ``metadata.csv`` is not strictly required. User can provide additional information with it, i.e. authors, title, journal and publish_time, mapping to each JSON files by every sha value. ``cord_uid`` serves unique values serve as the entry identity. Time sensitive entry, is advised to have ``publish_time`` value in Date format. Other values, General format is recommended.

Sample of ``metadata.csv`` entry: 
    =====================       =====================
    Column Names                Column Values 
    ---------------------       --------------------- 
    cord_uid                    zjufx4fo                
    sha                         b2897e1277f56641193a6db73825f707eed3e4c9  
    source_x                    PMC                   
    title                       Sequence requirements for RNA strand transfer during nidovirus ... 
    doi                         10.1093/emboj/20.24.7220         
    pmcid                       PMC125340                
    pubmed_id                   11742998                
    license                     unk                   
    abstract                    Nidovirus subgenomic mRNAs contain a leader sequence derived ...
    publish_time                2001-12-17             
    =====================       =====================
    
    
    
Dataset structure example #2:

.. code-block:: text

    /MedQuAD.zip
    │
    ├──FOLDER_NAME_1                                              # first level folder
    │  └──FOLDER_NAME_2                                           # second level folder, not necessarily to be included
    │      └──FOLDER_NAME_3                                       # third level folder, not necessarily to be included
    │           ├── 003d2e515e1aaf0052769953e8.xml             # xml file name is a random combination of either alphabets/numbers or both
    │           ├── 00a40758bdd.xml
    │           ...
    │
    ├──FOLDER_NAME_4                                              # first level folder
    │  ├── 0015023cc06b5332b3.xml
    │  ├── 001b4a31684c8fc6e2cfbb70304c429.xml
    │  ...
    ...


.. note::

    - For following `.xml` sample, model would only take `Question` and `Answer` fields into the question answering processing.
    - Each xml file contains multiple <QAPair>. Each <QAPair> contains one question and its answer. 
    
Sample `.xml` file:

.. code-block:: text

     <?xml version="1.0" encoding="UTF-8"?>
     <Document id="000001" source="A_source_here" url="An_url_here">
     ...
     <QAPairs>
      <QAPair pid="1">                                                           # pair #1
        <Question qid="000001-1" qtype=" "> A question here ... </Question>      # question #1
        <Answer> An answer here ... </Answer>                                    # answer of question #1
      </QAPair>
      ...                                                                        # multiple subsequent pairs
     </QAPairs>
     </Document>




Query Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

    - The pretrained model should be fine-tuned with a dataset first to adapt to particular question domains when necessary. 
    - Otherwise, following the question, input should contain relevant information (context paragraph or candidate answers, or both), whether or not addresses the question. 

Query is in JSON format. It could be a <str list> of a single question in ``questions`` field. While the relevant information as additional paragraph are provided in query, the question always comes first, followed by additional paragraph. We use “\n” separators between the question and its paragraph of the input. Model will only read the ``questions`` field. 

.. code-block:: text

    {
     'questions': ['At what speed did the turbine operate? \n (Nikola_Tesla) On his 50th birthday in 1906, .... several of his bladeless turbine engines were tested at 100–5,000 hp.',
                  'What does Paul McCartney think about his music? \n LAS VEGAS, Nevada (CNN) -- Former Beatles Paul McCartney and Ringo Starr clowned around and marveled at their band's amazing impact in an interview Tuesday on CNN's "Larry King Live."   ... McCartney said the early Beatles knew they were a good band and were pretty sure of themselves, but Starr said, "We thought we'd be really big in Liverpool."  ',
                  'The author tells us that to succeed in a project you are in charge of, you should   _  . \n  (A) make everyone work for you (B) get everyone willing to help you (C) let people know you have the final say (D) keep sending out orders to them \n If you're in charge of  a project, the key to success is getting everyone to want to help you. ...  You and your team can discover the answers to problems together. ',
                  'is the isle of man a part of great britain? \n (Isle of Man) In 1266, the island became part of Scotland under the Treaty of Perth, after being ruled by Norway.'
                              ],
      'answers':['16,000 rpm',
                      'very good',
                      'get everyone willing to help you',
                      'no'
                    ]
    }

Prediction Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The output is in JSON format.

.. code-block:: text

         {'answers':['16,000 rpm',
                     'very good',
                     'get everyone willing to help you',
                     'no'
                     ]}



IMAGE_CLASSIFICATION
--------------------------------------------------------------------

Dataset Format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:IMAGE_FILES`

- There is only 1 tag column of ``class``, corresponding to the class of the image as an integer from ``0`` to ``k - 1``, where ``k`` is the total no. of classes.
- The train & validation dataset's images should be have the same dimensions ``W x H`` and same total no. of classes.

An example:

.. code-block:: text

    path,class
    image-0-of-class-0.png,0
    image-1-of-class-0.png,0
    ...
    image-0-of-class-1.png,1
    ...
    image-99-of-class-9.png,9
    
.. note::

    You can refer to and run `./examples/datasets/image_files/load_folder_format.py <https://github.com/nusdbsystem/singa-auto/tree/master/examples/datasets/load_folder_format.py>`_
    for converting *directories of images* to SINGA-Auto's ``IMAGE_CLASSIFICATION`` format. 


Query Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A ``W x H x 3`` 3D array representing a *RGB* version of the query image.
The query image can be of *any dimensions*.

Prediction Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A size-``k`` array of floats, representing the probabilities of each class, by index, from ``0`` to ``k-1``.
For example, the float at index 0 corresponds to the probability of class 0.


POS_TAGGING
--------------------------------------------------------------------

Dataset Format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:CORPUS`

- Sentences are delimited by  ``\n`` tokens.
- There is only 1 tag column of ``tag`` corresponding to the POS tag of the token as an integer from ``0`` to ``k-1``.

An example:

.. code-block:: text

    token       tag
    Two         3
    leading     2
    ...
    line-item   1
    veto        5
    .           4
    \n          0
    Professors  6
    Philip      6
    ...
    previous    1
    presidents  8   
    .           4
    \n          0


Query Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An array of strings representing a sentence as a list of tokens in that sentence.

Prediction Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A array of integers representing the list of predicted tag for each token, in sequence, for the sentence.

TABULAR_CLASSIFICATION
--------------------------------------------------------------------

Dataset Type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:TABULAR`

The following optional train arguments are supported:

    =====================       =====================
    **Train Argument**          **Description**
    ---------------------       ---------------------        
    ``features``                List of feature columns' names as a list of strings (defaults to first ``N-1`` columns in the CSV file)
    ``target``                  Target column name as a string (defaults to the *last* column in the CSV file)
    =====================       =====================

The train & validation datasets should have the same columns. 

Query Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An size-``N-1`` dictionary representing feature-value pairs.

Prediction Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A size-``k`` list of floats, representing the probabilities of each class from ``0`` to ``k-1`` for the target column.


TABULAR_REGRESSION
--------------------------------------------------------------------

Dataset Type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:TABULAR`

The following optional train arguments are supported:

    =====================       =====================
    **Train Argument**          **Description**
    ---------------------       ---------------------        
    ``features``                List of feature columns' names as a list of strings (defaults to first ``N-1`` columns in the CSV file)
    ``target``                  Target column name as a string (defaults to the *last* column in the CSV file)
    =====================       =====================
    
The train & validation datasets should have the same columns. 

An example of the dataset follows:

.. code-block:: text

    density,bodyfat,age,weight,height,neck,chest,abdomen,hip,thigh,knee,ankle,biceps,forearm,wrist
    1.0708,12.3,23,154.25,67.75,36.2,93.1,85.2,94.5,59,37.3,21.9,32,27.4,17.1
    1.0853,6.1,22,173.25,72.25,38.5,93.6,83,98.7,58.7,37.3,23.4,30.5,28.9,18.2
    1.0414,25.3,22,154,66.25,34,95.8,87.9,99.2,59.6,38.9,24,28.8,25.2,16.6
    ...

Query Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An size-``N-1`` dictionary representing feature-value pairs.

Prediction Format 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A float, representing the value of the target column.


SPEECH_RECOGNITION
--------------------------------------------------------------------

Speech recognition for the *English* language.

Dataset Type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`dataset-type:AUDIO_FILES`

The ``audios.csv`` should be of a `.CSV <https://en.wikipedia.org/wiki/Comma-separated_values>`_
format with 3 columns of ``wav_filename``, ``wav_filesize`` and ``transcript``.

For each row,

    ``wav_filename`` should be a file path to a ``.wav`` audio file within the archive, relative to the root of the directory.
    Each audio file's sample rate must equal to 16kHz.

    ``wav_filesize`` should be an integer representing the size of the ``.wav`` audio file, in number of bytes.

    ``transcript`` should be a string of the true transcript for the audio file. Transcripts should only contain the following alphabets:

        ::

            a
            b
            c
            d
            e
            f
            g
            h
            i
            j
            k
            l
            m
            n
            o
            p
            q
            r
            s
            t
            u
            v
            w
            x
            y
            z

            
            '

 An example of ``audios.csv`` follows:

.. code-block:: text

    wav_filename,wav_filesize,transcript
    6930-81414-0000.wav,412684,audio transcript one
    6930-81414-0001.wav,559564,audio transcript two
    ...
    672-122797-0005.wav,104364,audio transcript one thousand
    ...
    1995-1837-0001.wav,279404,audio transcript three thousand


Query Format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A `Base64-encoded <https://en.wikipedia.org/wiki/Base64>`_ string of the bytes of the audio as a 16kHz `.wav` file


Prediction Format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A string, representing the predicted transcript for the audio.
